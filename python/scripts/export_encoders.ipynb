{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scripts for Exporting PyTorch Models to ONNX and CoreML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade \"uform[torch]\" coremltools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uform\n",
    "from PIL import Image\n",
    "\n",
    "model, processor = uform.get_model('unum-cloud/uform-vl-english-small')\n",
    "text = 'a small red panda in a zoo'\n",
    "image = Image.open('../../assets/unum.png')\n",
    "\n",
    "image_data = processor.preprocess_image(image)\n",
    "text_data = processor.preprocess_text(text)\n",
    "\n",
    "image_features, image_embedding = model.encode_image(image_data, return_features=True)\n",
    "text_features, text_embedding = model.encode_text(text_data, return_features=True)\n",
    "\n",
    "image_features.shape, text_features.shape, image_embedding.shape, text_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.text_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.image_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `model` is your loaded model with image_encoder and text_encoder attributes\n",
    "for name, module in model.image_encoder.named_children():\n",
    "    print(f\"First layer of image_encoder: {name}\")\n",
    "    break  # We break after the first layer\n",
    "\n",
    "for name, module in model.text_encoder.named_children():\n",
    "    print(f\"First layer of text_encoder: {name}\")\n",
    "    break  # We break after the first layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoreML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import coremltools as ct\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_input = ct.TensorType(name=\"input\", shape=image_data.shape)\n",
    "text_input = ct.TensorType(name=\"input_ids\", shape=text_data[\"input_ids\"].shape)\n",
    "text_attention_input = ct.TensorType(name=\"attention_mask\", shape=text_data[\"attention_mask\"].shape)\n",
    "text_features = ct.TensorType(name=\"features\")\n",
    "text_embeddings = ct.TensorType(name=\"embeddings\")\n",
    "image_features = ct.TensorType(name=\"features\")\n",
    "image_embeddings = ct.TensorType(name=\"embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = model.image_encoder\n",
    "module.eval()\n",
    "module.return_features = True\n",
    "\n",
    "traced_script_module = torch.jit.trace(module, example_inputs=image_data)\n",
    "traced_script_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coreml_model = ct.convert(\n",
    "    traced_script_module, source=\"pytorch\",\n",
    "    inputs=[image_input], outputs=[image_features, image_embeddings],\n",
    "    convert_to='mlprogram', compute_precision=ct.precision.FLOAT32)\n",
    "\n",
    "coreml_model.author = 'Unum Cloud'\n",
    "coreml_model.license = 'Apache 2.0'\n",
    "coreml_model.short_description = 'Pocket-Sized Multimodal AI for Content Understanding'\n",
    "coreml_model.save(\"../uform-vl-english-small-image.mlpackage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = model.text_encoder\n",
    "module.eval()\n",
    "module.return_features = True\n",
    "\n",
    "traced_script_module = torch.jit.trace(module, example_inputs=[text_data['input_ids'], text_data['attention_mask']])\n",
    "traced_script_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coreml_model = ct.convert(\n",
    "    traced_script_module, source=\"pytorch\",\n",
    "    inputs=[text_input, text_attention_input], outputs=[text_features, text_embeddings],\n",
    "    convert_to='mlprogram', compute_precision=ct.precision.FLOAT32)\n",
    "\n",
    "coreml_model.author = 'Unum Cloud'\n",
    "coreml_model.license = 'Apache 2.0'\n",
    "coreml_model.short_description = 'Pocket-Sized Multimodal AI for Content Understanding'\n",
    "coreml_model.save(\"../uform-vl-english-small-text.mlpackage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch\n",
    "\n",
    "Let's ensure that the input layers and the model itself works fine in `f16` half-precision, so that the model is lighter and easier to download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from safetensors import safe_open\n",
    "from safetensors.torch import save_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.image_encoder.eval()\n",
    "model.image_encoder.to(dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.image_encoder.state_dict(), 'image.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file(model.image_encoder.state_dict(), \"image.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.text_encoder.eval()\n",
    "model.text_encoder.to(dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.text_encoder.state_dict(), 'text.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file(model.text_encoder.state_dict(), \"text.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features, image_embedding = model.encode_image(image_data.to(dtype=torch.bfloat16), return_features=True)\n",
    "text_features, text_embedding = model.encode_text(text_data, return_features=True)\n",
    "\n",
    "image_features.shape, text_features.shape, image_embedding.shape, text_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli upload unum-cloud/uform2-vl-english-small image.safetensors image.safetensors\n",
    "!huggingface-cli upload unum-cloud/uform2-vl-english-small text.safetensors text.safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli upload unum-cloud/uform2-vl-english-small image.pt image.pt\n",
    "!huggingface-cli upload unum-cloud/uform2-vl-english-small text.pt text.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install onnx onnxconverter-common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.onnx import export as onnx_export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't immediately export to `bfloat16` as it's not supported by ONNX, but we also can't export to `float16`, as the forward pass (that will be traced) is gonna fail. So let's export to `float32` ONNX file first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = model.text_encoder\n",
    "module.eval()\n",
    "module.return_features = True\n",
    "module.to(dtype=torch.float32)\n",
    "\n",
    "onnx_export(\n",
    "    module,\n",
    "    (text_data[\"input_ids\"], text_data[\"attention_mask\"]), \n",
    "    \"text.onnx\", \n",
    "    export_params=True,\n",
    "    opset_version=15,\n",
    "    do_constant_folding=True,\n",
    "    input_names = ['input_ids', 'attention_mask'], \n",
    "    output_names = ['features', 'embeddings'],\n",
    "    dynamic_axes={\n",
    "        'input_ids' : {0 : 'batch_size'}, \n",
    "        'attention_mask' : {0 : 'batch_size'}, \n",
    "        'features' : {0 : 'batch_size'}, \n",
    "        'embeddings' : {0 : 'batch_size'}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use [additional ONNX tooling](https://onnxruntime.ai/docs/performance/model-optimizations/float16.html#mixed-precision) to convert to half-precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "from onnxconverter_common import float16\n",
    "\n",
    "module = onnx.load(\"text.onnx\")\n",
    "module_fp16 = float16.convert_float_to_float16(module)\n",
    "onnx.save(module_fp16, \"text.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now repeat the same for images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = model.image_encoder\n",
    "module.eval()\n",
    "module.return_features = True\n",
    "module.to(dtype=torch.float32)\n",
    "\n",
    "torch.onnx.export(\n",
    "    module,\n",
    "    image_data, \n",
    "    \"image.onnx\", \n",
    "    export_params=True,\n",
    "    opset_version=15,\n",
    "    do_constant_folding=True,\n",
    "    input_names = ['input'], \n",
    "    output_names = ['features', 'embeddings'],\n",
    "    dynamic_axes={\n",
    "        'input' : {0 : 'batch_size'},\n",
    "        'features' : {0 : 'batch_size'},\n",
    "        'embeddings' : {0 : 'batch_size'}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "from onnxconverter_common import float16\n",
    "\n",
    "module = onnx.load(\"image.onnx\")\n",
    "module_fp16 = float16.convert_float_to_float16(module)\n",
    "onnx.save(module_fp16, \"image.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli upload unum-cloud/uform2-vl-english-small image.onnx image.onnx\n",
    "!huggingface-cli upload unum-cloud/uform2-vl-english-small text.onnx text.onnx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
