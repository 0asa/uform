{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scripts for Exporting PyTorch Models to ONNX and CoreML\n",
    "\n",
    "Depending on the backend, we prefer different qunatization schemes.\n",
    "\n",
    "- For ONNX we use `int8` quantization.\n",
    "- For PyTorch we use `bfloat16` quantization.\n",
    "- For CoreML we use `float32` representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade \"uform[torch]\" coremltools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "model_name = \"uform-vl-english-small\"\n",
    "output_directory = \"../../\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uform\n",
    "from PIL import Image\n",
    "\n",
    "model, processor = uform.get_model('unum-cloud/uform-vl-english-small')\n",
    "text = 'a small red panda in a zoo'\n",
    "image = Image.open('../../assets/unum.png')\n",
    "\n",
    "image_data = processor.preprocess_image(image)\n",
    "text_data = processor.preprocess_text(text)\n",
    "\n",
    "image_features, image_embedding = model.encode_image(image_data, return_features=True)\n",
    "text_features, text_embedding = model.encode_text(text_data, return_features=True)\n",
    "\n",
    "image_features.shape, text_features.shape, image_embedding.shape, text_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.text_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.image_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `model` is your loaded model with image_encoder and text_encoder attributes\n",
    "for name, module in model.image_encoder.named_children():\n",
    "    print(f\"First layer of image_encoder: {name}\")\n",
    "    break  # We break after the first layer\n",
    "\n",
    "for name, module in model.text_encoder.named_children():\n",
    "    print(f\"First layer of text_encoder: {name}\")\n",
    "    break  # We break after the first layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoreML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import coremltools as ct\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = ct.precision.FLOAT32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CoreML Tools provides a way to convert ONNX models to CoreML models. This script demonstrates how to convert an ONNX model to a CoreML model. For that, we need to provide an example input, and the tensor shapes will be inferred from that.\n",
    "\n",
    "```python\n",
    "        image_input = ct.TensorType(name=\"input\", shape=image_data.shape)\n",
    "        text_input = ct.TensorType(name=\"input_ids\", shape=text_data[\"input_ids\"].shape)\n",
    "        text_attention_input = ct.TensorType(name=\"attention_mask\", shape=text_data[\"attention_mask\"].shape)\n",
    "```\n",
    "\n",
    "That, however, will only work for batch-size one. To support larger batches, we need to override the input shapes.\n",
    "\n",
    "```python\n",
    "        ct.RangeDim(lower_bound=25, upper_bound=100, default=45)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generalize_first_dimensions(input_shape, upper_bound=64):\n",
    "    if upper_bound == 1:\n",
    "        return input_shape\n",
    "    input_shape = (ct.RangeDim(lower_bound=1, upper_bound=upper_bound, default=1),) + input_shape[1:]\n",
    "    return input_shape\n",
    "\n",
    "generalize_first_dimensions(image_data.shape), generalize_first_dimensions(text_data[\"input_ids\"].shape), generalize_first_dimensions(text_data[\"attention_mask\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_input = ct.TensorType(name=\"input\", shape=generalize_first_dimensions(image_data.shape, 1))\n",
    "text_input = ct.TensorType(name=\"input_ids\", shape=generalize_first_dimensions(text_data[\"input_ids\"].shape, 1))\n",
    "text_attention_input = ct.TensorType(name=\"attention_mask\", shape=generalize_first_dimensions(text_data[\"attention_mask\"].shape, 1))\n",
    "text_features = ct.TensorType(name=\"features\")\n",
    "text_embeddings = ct.TensorType(name=\"embeddings\")\n",
    "image_features = ct.TensorType(name=\"features\")\n",
    "image_embeddings = ct.TensorType(name=\"embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = model.image_encoder\n",
    "module.eval()\n",
    "module.return_features = True\n",
    "\n",
    "traced_script_module = torch.jit.trace(module, example_inputs=image_data)\n",
    "traced_script_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coreml_model = ct.convert(\n",
    "    traced_script_module, source=\"pytorch\",\n",
    "    inputs=[image_input], outputs=[image_features, image_embeddings],\n",
    "    convert_to='mlprogram', compute_precision=precision)\n",
    "\n",
    "coreml_model.author = 'Unum Cloud'\n",
    "coreml_model.license = 'Apache 2.0'\n",
    "coreml_model.short_description = 'Pocket-Sized Multimodal AI for Content Understanding'\n",
    "coreml_model.save(os.path.join(output_directory, \"image_encoder.mlpackage\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = model.text_encoder\n",
    "module.eval()\n",
    "module.return_features = True\n",
    "\n",
    "traced_script_module = torch.jit.trace(module, example_inputs=[text_data['input_ids'], text_data['attention_mask']])\n",
    "traced_script_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coreml_model = ct.convert(\n",
    "    traced_script_module, source=\"pytorch\",\n",
    "    inputs=[text_input, text_attention_input], outputs=[text_features, text_embeddings],\n",
    "    convert_to='mlprogram', compute_precision=precision)\n",
    "\n",
    "coreml_model.author = 'Unum Cloud'\n",
    "coreml_model.license = 'Apache 2.0'\n",
    "coreml_model.short_description = 'Pocket-Sized Multimodal AI for Content Understanding'\n",
    "coreml_model.save(os.path.join(output_directory, \"text_encoder.mlpackage\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch\n",
    "\n",
    "Let's ensure:\n",
    "\n",
    "- the `model.text_encoder` inputs are called `input_ids` and `attention_mask`, and outputs are `embeddings` and `features`.\n",
    "- the `model.image_encoder` input is called `input`, and outputs are `embeddings` and `features`.\n",
    "- the model itself works fine in `f16` half-precision, so that the model is lighter and easier to download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from safetensors import safe_open\n",
    "from safetensors.torch import save_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.image_encoder.eval()\n",
    "model.image_encoder.to(dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.image_encoder.state_dict(), os.path.join(output_directory, \"image_encoder.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file(model.image_encoder.state_dict(), os.path.join(output_directory, \"image_encoder.safetensors\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.text_encoder.eval()\n",
    "model.text_encoder.to(dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.text_encoder.state_dict(), os.path.join(output_directory, \"text_encoder.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file(model.text_encoder.state_dict(), os.path.join(output_directory, \"text_encoder.safetensors\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features, image_embedding = model.encode_image(image_data.to(dtype=torch.bfloat16), return_features=True)\n",
    "text_features, text_embedding = model.encode_text(text_data, return_features=True)\n",
    "\n",
    "image_features.shape, text_features.shape, image_embedding.shape, text_embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install onnx onnxconverter-common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.onnx import export as onnx_export\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't immediately export to `bfloat16` as it's not supported by ONNX, but we also can't export to `float16`, as the forward pass (that will be traced) is gonna fail. So let's export to `float32` ONNX file first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = model.text_encoder\n",
    "module.eval()\n",
    "module.return_features = True\n",
    "module.to(dtype=torch.float32)\n",
    "\n",
    "onnx_export(\n",
    "    module,\n",
    "    (text_data[\"input_ids\"], text_data[\"attention_mask\"]), \n",
    "    os.path.join(output_directory, \"text_encoder.onnx\"), \n",
    "    export_params=True,\n",
    "    opset_version=15,\n",
    "    do_constant_folding=True,\n",
    "    input_names = ['input_ids', 'attention_mask'], \n",
    "    output_names = ['features', 'embeddings'],\n",
    "    dynamic_axes={\n",
    "        'input_ids' : {0 : 'batch_size'}, \n",
    "        'attention_mask' : {0 : 'batch_size'}, \n",
    "        'features' : {0 : 'batch_size'}, \n",
    "        'embeddings' : {0 : 'batch_size'}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now repeat the same for images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = model.image_encoder\n",
    "module.eval()\n",
    "module.return_features = True\n",
    "module.to(dtype=torch.float32)\n",
    "\n",
    "torch.onnx.export(\n",
    "    module,\n",
    "    image_data, \n",
    "    os.path.join(output_directory, \"image_encoder.onnx\"), \n",
    "    export_params=True,\n",
    "    opset_version=15,\n",
    "    do_constant_folding=True,\n",
    "    input_names = ['input'], \n",
    "    output_names = ['features', 'embeddings'],\n",
    "    dynamic_axes={\n",
    "        'input' : {0 : 'batch_size'},\n",
    "        'features' : {0 : 'batch_size'},\n",
    "        'embeddings' : {0 : 'batch_size'}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantizing to `float16`\n",
    "\n",
    "Let's use [additional ONNX tooling](https://onnxruntime.ai/docs/performance/model-optimizations/float16.html#mixed-precision) to convert to half-precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "from onnxconverter_common import float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = os.path.join(output_directory, \"text_encoder.onnx\")\n",
    "module = onnx.load(module_path)\n",
    "module_fp16 = float16.convert_float_to_float16(module)\n",
    "onnx.save(module_fp16, module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = os.path.join(output_directory, \"image_encoder.onnx\")\n",
    "module = onnx.load(module_path)\n",
    "module_fp16 = float16.convert_float_to_float16(module)\n",
    "onnx.save(module_fp16, module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantizing to `uint8`\n",
    "\n",
    "We can further quantize the model into `uint8` using ONNX quantization tools.\n",
    "The `int8` is default variant, but [some of the operators don't support it](https://github.com/microsoft/onnxruntime/issues/15888)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from onnxruntime.quantization import quantize_dynamic, QuantType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = os.path.join(output_directory, \"text_encoder.onnx\")\n",
    "quantize_dynamic(module_path, module_path, weight_type=QuantType.QUInt8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = os.path.join(output_directory, \"image_encoder.onnx\")\n",
    "quantize_dynamic(module_path, module_path, weight_type=QuantType.QUInt8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure that all the text inputs are integers of identical type - `int32`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import os\n",
    "from onnx import helper\n",
    "\n",
    "# Load the ONNX model\n",
    "module_path = os.path.join(output_directory, \"text_encoder.onnx\")\n",
    "module = onnx.load(module_path)\n",
    "\n",
    "# Get the module's graph\n",
    "graph = module.graph\n",
    "\n",
    "# Iterate through the inputs and update the data type of `input_ids`\n",
    "for input_tensor in graph.input:\n",
    "    # Check if this is the tensor we want to change\n",
    "    if input_tensor.name == 'input_ids' or input_tensor.name == 'attention_mask':\n",
    "        # Get the tensor type information\n",
    "        tensor_type = input_tensor.type.tensor_type\n",
    "        # Set the element type to INT32 (int32's enum value in onnx is 6)\n",
    "        tensor_type.elem_type = onnx.TensorProto.INT32\n",
    "\n",
    "# Optionally, check that the module is still valid\n",
    "onnx.checker.check_model(module)\n",
    "\n",
    "# Save the modified module\n",
    "onnx.save(module, module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the following function to print and validate the input and output types of the ONNX model files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_inputs_and_outputs(onnx_model_path):\n",
    "    model = onnx.load(onnx_model_path)\n",
    "\n",
    "    # Get the model's graph\n",
    "    graph = model.graph\n",
    "\n",
    "    # Print input information\n",
    "    print(\"Model Inputs:\")\n",
    "    for input_tensor in graph.input:\n",
    "        tensor_type = input_tensor.type.tensor_type\n",
    "        # Get the element type (data type)\n",
    "        elem_type = tensor_type.elem_type\n",
    "        # Convert numeric type to readable format\n",
    "        readable_type = onnx.TensorProto.DataType.Name(elem_type)\n",
    "        # Get tensor shape\n",
    "        shape = [dim.dim_value for dim in tensor_type.shape.dim]\n",
    "        print(f\"Name: {input_tensor.name}, Type: {readable_type}, Shape: {shape}\")\n",
    "\n",
    "    # Print output information similarly if needed\n",
    "    print(\"\\nModel Outputs:\")\n",
    "    for output_tensor in graph.output:\n",
    "        tensor_type = output_tensor.type.tensor_type\n",
    "        elem_type = tensor_type.elem_type\n",
    "        readable_type = onnx.TensorProto.DataType.Name(elem_type)\n",
    "        shape = [dim.dim_value for dim in tensor_type.shape.dim]\n",
    "        print(f\"Name: {output_tensor.name}, Type: {readable_type}, Shape: {shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the runtime can actually load those models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "session_options = ort.SessionOptions()\n",
    "session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = os.path.join(output_directory, \"text_encoder.onnx\")\n",
    "session = ort.InferenceSession(module_path, sess_options=session_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = os.path.join(output_directory, \"image_encoder.onnx\")\n",
    "session = ort.InferenceSession(module_path, sess_options=session_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload to Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli upload unum-cloud/uform3-image-text-english-small ../../image_encoder.onnx image_encoder.onnx\n",
    "!huggingface-cli upload unum-cloud/uform3-image-text-english-small ../../text_encoder.onnx text_encoder.onnx\n",
    "!huggingface-cli upload unum-cloud/uform3-image-text-english-small ../../image_encoder.safetensors image_encoder.safetensors\n",
    "!huggingface-cli upload unum-cloud/uform3-image-text-english-small ../../text_encoder.safetensors text_encoder.safetensors\n",
    "!huggingface-cli upload unum-cloud/uform3-image-text-english-small ../../image_encoder.pt image_encoder.pt\n",
    "!huggingface-cli upload unum-cloud/uform3-image-text-english-small ../../text_encoder.pt text_encoder.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
