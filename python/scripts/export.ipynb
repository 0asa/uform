{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scripts for Exporting PyTorch Models to ONNX and CoreML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade \"uform[torch]\" coremltools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "model_name = \"uform-vl-english-small\"\n",
    "output_directory = \"../../\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uform\n",
    "from PIL import Image\n",
    "\n",
    "model, processor = uform.get_model('unum-cloud/' + model_name)\n",
    "text = 'a small red panda in a zoo'\n",
    "image = Image.open('../../assets/unum.png')\n",
    "\n",
    "image_data = processor.preprocess_image(image)\n",
    "text_data = processor.preprocess_text(text)\n",
    "\n",
    "image_features, image_embedding = model.encode_image(image_data, return_features=True)\n",
    "text_features, text_embedding = model.encode_text(text_data, return_features=True)\n",
    "\n",
    "image_features.shape, text_features.shape, image_embedding.shape, text_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.text_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.image_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `model` is your loaded model with image_encoder and text_encoder attributes\n",
    "for name, module in model.image_encoder.named_children():\n",
    "    print(f\"First layer of image_encoder: {name}\")\n",
    "    break  # We break after the first layer\n",
    "\n",
    "for name, module in model.text_encoder.named_children():\n",
    "    print(f\"First layer of text_encoder: {name}\")\n",
    "    break  # We break after the first layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch\n",
    "\n",
    "Let's ensure:\n",
    "\n",
    "- the `model.text_encoder` inputs are called `input_ids` and `attention_mask`, and outputs are `embeddings` and `features`.\n",
    "- the `model.image_encoder` input is called `input`, and outputs are `embeddings` and `features`.\n",
    "- the model itself works fine in `f16` half-precision, so that the model is lighter and easier to download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(name for name, _ in model.text_encoder.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify input and output names for text_encoder\n",
    "text_encoder_input_names = [name for name, _ in model.text_encoder.named_parameters()]\n",
    "assert 'input_ids' in text_encoder_input_names, \"input_ids not found in text_encoder inputs\"\n",
    "assert 'attention_mask' in text_encoder_input_names, \"attention_mask not found in text_encoder inputs\"\n",
    "\n",
    "text_encoder_output_names = [name for name, _ in model.text_encoder.named_modules()]\n",
    "assert 'embeddings' in text_encoder_output_names, \"embeddings not found in text_encoder outputs\"\n",
    "assert 'features' in text_encoder_output_names, \"features not found in text_encoder outputs\"\n",
    "\n",
    "# Verify input and output names for image_encoder\n",
    "image_encoder_input_names = [name for name, _ in model.image_encoder.named_parameters()]\n",
    "assert 'input' in image_encoder_input_names, \"input not found in image_encoder inputs\"\n",
    "\n",
    "image_encoder_output_names = [name for name, _ in model.image_encoder.named_modules()]\n",
    "assert 'embeddings' in image_encoder_output_names, \"embeddings not found in image_encoder outputs\"\n",
    "assert 'features' in image_encoder_output_names, \"features not found in image_encoder outputs\"\n",
    "\n",
    "# Ensure the model can be converted to f16 half-precision\n",
    "try:\n",
    "    model.half()  # Convert to half precision\n",
    "    print(\"Model successfully converted to half precision (f16).\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while converting the model to half precision: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoreML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import coremltools as ct\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = ct.precision.FLOAT32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CoreML Tools provides a way to convert ONNX models to CoreML models. This script demonstrates how to convert an ONNX model to a CoreML model. For that, we need to provide an example input, and the tensor shapes will be inferred from that.\n",
    "\n",
    "```python\n",
    "        image_input = ct.TensorType(name=\"input\", shape=image_data.shape)\n",
    "        text_input = ct.TensorType(name=\"input_ids\", shape=text_data[\"input_ids\"].shape)\n",
    "        text_attention_input = ct.TensorType(name=\"attention_mask\", shape=text_data[\"attention_mask\"].shape)\n",
    "```\n",
    "\n",
    "That, however, will only work for batch-size one. To support larger batches, we need to override the input shapes.\n",
    "\n",
    "```python\n",
    "        ct.RangeDim(lower_bound=25, upper_bound=100, default=45)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generalize_first_dimensions(input_shape, upper_bound=64):\n",
    "    if upper_bound == 1:\n",
    "        return input_shape\n",
    "    input_shape = (ct.RangeDim(lower_bound=1, upper_bound=upper_bound, default=1),) + input_shape[1:]\n",
    "    return input_shape\n",
    "\n",
    "generalize_first_dimensions(image_data.shape), generalize_first_dimensions(text_data[\"input_ids\"].shape), generalize_first_dimensions(text_data[\"attention_mask\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_input = ct.TensorType(name=\"input\", shape=generalize_first_dimensions(image_data.shape, 1))\n",
    "text_input = ct.TensorType(name=\"input_ids\", shape=generalize_first_dimensions(text_data[\"input_ids\"].shape, 1))\n",
    "text_attention_input = ct.TensorType(name=\"attention_mask\", shape=generalize_first_dimensions(text_data[\"attention_mask\"].shape, 1))\n",
    "text_features = ct.TensorType(name=\"features\")\n",
    "text_embeddings = ct.TensorType(name=\"embeddings\")\n",
    "image_features = ct.TensorType(name=\"features\")\n",
    "image_embeddings = ct.TensorType(name=\"embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = model.image_encoder\n",
    "module.eval()\n",
    "module.return_features = True\n",
    "\n",
    "traced_script_module = torch.jit.trace(module, example_inputs=image_data)\n",
    "traced_script_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coreml_model = ct.convert(\n",
    "    traced_script_module, source=\"pytorch\",\n",
    "    inputs=[image_input], outputs=[image_features, image_embeddings],\n",
    "    convert_to='mlprogram', compute_precision=precision)\n",
    "\n",
    "coreml_model.author = 'Unum Cloud'\n",
    "coreml_model.license = 'Apache 2.0'\n",
    "coreml_model.short_description = 'Pocket-Sized Multimodal AI for Content Understanding'\n",
    "coreml_model.save(os.path.join(output_directory, model_name + \"-image.mlpackage\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = model.text_encoder\n",
    "module.eval()\n",
    "module.return_features = True\n",
    "\n",
    "traced_script_module = torch.jit.trace(module, example_inputs=[text_data['input_ids'], text_data['attention_mask']])\n",
    "traced_script_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coreml_model = ct.convert(\n",
    "    traced_script_module, source=\"pytorch\",\n",
    "    inputs=[text_input, text_attention_input], outputs=[text_features, text_embeddings],\n",
    "    convert_to='mlprogram', compute_precision=precision)\n",
    "\n",
    "coreml_model.author = 'Unum Cloud'\n",
    "coreml_model.license = 'Apache 2.0'\n",
    "coreml_model.short_description = 'Pocket-Sized Multimodal AI for Content Understanding'\n",
    "coreml_model.save(os.path.join(output_directory, model_name + \"-text.mlpackage\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
