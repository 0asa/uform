{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scripts for Exporting PyTorch Models to ONNX and CoreML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade \"uform[torch]\" coremltools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/av/miniconda3/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: dlopen(/Users/av/miniconda3/lib/python3.10/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c106detail19maybe_wrap_dim_slowExxb\n",
      "  Referenced from: <0B637046-A38B-3A5C-80C6-E847C27DCCD5> /Users/av/miniconda3/lib/python3.10/site-packages/torchvision/image.so\n",
      "  Expected in:     <3AE92490-D363-3FD7-8532-CB6F5F795BC8> /Users/av/miniconda3/lib/python3.10/site-packages/torch/lib/libc10.dylib\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fadffc0299c04e249fd4f7a5b40ba0af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 197, 384]),\n",
       " torch.Size([1, 64, 768]),\n",
       " torch.Size([1, 256]),\n",
       " torch.Size([1, 256]))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import uform\n",
    "from PIL import Image\n",
    "\n",
    "model, processor = uform.get_model('unum-cloud/uform-vl-english-small')\n",
    "text = 'a small red panda in a zoo'\n",
    "image = Image.open('../../assets/unum.png')\n",
    "\n",
    "image_data = processor.preprocess_image(image)\n",
    "text_data = processor.preprocess_text(text)\n",
    "\n",
    "image_features, image_embedding = model.encode_image(image_data, return_features=True)\n",
    "text_features, text_embedding = model.encode_text(text_data, return_features=True)\n",
    "\n",
    "image_features.shape, text_features.shape, image_embedding.shape, text_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextEncoder(model_type='bert', dim=768, context_dim=384, vocab_size=30522, padding_idx=0, num_layers=4, num_heads=12, embedding_dim=256, multimodal_layers_ids=[2, 3], head_one_neuron=False, pooling='cls', max_position_embeddings=64, dropout_prob=0.1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.text_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisualEncoder(dim=384, patch_size=16, image_size=224, num_layers=12, num_heads=6, embedding_dim=256, pooling='cls', num_reg_tokens=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.image_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First layer of image_encoder: patch_embed\n",
      "First layer of text_encoder: word_embeddings\n"
     ]
    }
   ],
   "source": [
    "# Assuming `model` is your loaded model with image_encoder and text_encoder attributes\n",
    "for name, module in model.image_encoder.named_children():\n",
    "    print(f\"First layer of image_encoder: {name}\")\n",
    "    break  # We break after the first layer\n",
    "\n",
    "for name, module in model.text_encoder.named_children():\n",
    "    print(f\"First layer of text_encoder: {name}\")\n",
    "    break  # We break after the first layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoreML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scikit-learn version 1.2.1 is not supported. Minimum required version: 0.17. Maximum required version: 1.1.2. Disabling scikit-learn conversion API.\n",
      "Torch version 2.1.1 has not been tested with coremltools. You may run into unexpected errors. Torch 2.1.0 is the most recent version that has been tested.\n"
     ]
    }
   ],
   "source": [
    "import coremltools as ct\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_input = ct.TensorType(name=\"input\", shape=image_data.shape)\n",
    "text_input = ct.TensorType(name=\"input_ids\", shape=text_data[\"input_ids\"].shape)\n",
    "text_attention_input = ct.TensorType(name=\"attention_mask\", shape=text_data[\"attention_mask\"].shape)\n",
    "text_features = ct.TensorType(name=\"features\")\n",
    "text_embeddings = ct.TensorType(name=\"embeddings\")\n",
    "image_features = ct.TensorType(name=\"features\")\n",
    "image_embeddings = ct.TensorType(name=\"embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisualEncoder(\n",
       "  original_name=VisualEncoder\n",
       "  (patch_embed): Conv2d(original_name=Conv2d)\n",
       "  (blocks): Sequential(\n",
       "    original_name=Sequential\n",
       "    (0): VisualEncoderBlock(\n",
       "      original_name=VisualEncoderBlock\n",
       "      (norm1): LayerNorm(original_name=LayerNorm)\n",
       "      (attn): Attention(\n",
       "        original_name=Attention\n",
       "        (query): Linear(original_name=Linear)\n",
       "        (key): Linear(original_name=Linear)\n",
       "        (value): Linear(original_name=Linear)\n",
       "        (out): Linear(original_name=Linear)\n",
       "      )\n",
       "      (ls1): LayerScale(original_name=LayerScale)\n",
       "      (norm2): LayerNorm(original_name=LayerNorm)\n",
       "      (mlp): MLP(\n",
       "        original_name=MLP\n",
       "        (hidden_layer): Linear(original_name=Linear)\n",
       "        (output_layer): Linear(original_name=Linear)\n",
       "      )\n",
       "      (ls2): LayerScale(original_name=LayerScale)\n",
       "    )\n",
       "    (1): VisualEncoderBlock(\n",
       "      original_name=VisualEncoderBlock\n",
       "      (norm1): LayerNorm(original_name=LayerNorm)\n",
       "      (attn): Attention(\n",
       "        original_name=Attention\n",
       "        (query): Linear(original_name=Linear)\n",
       "        (key): Linear(original_name=Linear)\n",
       "        (value): Linear(original_name=Linear)\n",
       "        (out): Linear(original_name=Linear)\n",
       "      )\n",
       "      (ls1): LayerScale(original_name=LayerScale)\n",
       "      (norm2): LayerNorm(original_name=LayerNorm)\n",
       "      (mlp): MLP(\n",
       "        original_name=MLP\n",
       "        (hidden_layer): Linear(original_name=Linear)\n",
       "        (output_layer): Linear(original_name=Linear)\n",
       "      )\n",
       "      (ls2): LayerScale(original_name=LayerScale)\n",
       "    )\n",
       "    (2): VisualEncoderBlock(\n",
       "      original_name=VisualEncoderBlock\n",
       "      (norm1): LayerNorm(original_name=LayerNorm)\n",
       "      (attn): Attention(\n",
       "        original_name=Attention\n",
       "        (query): Linear(original_name=Linear)\n",
       "        (key): Linear(original_name=Linear)\n",
       "        (value): Linear(original_name=Linear)\n",
       "        (out): Linear(original_name=Linear)\n",
       "      )\n",
       "      (ls1): LayerScale(original_name=LayerScale)\n",
       "      (norm2): LayerNorm(original_name=LayerNorm)\n",
       "      (mlp): MLP(\n",
       "        original_name=MLP\n",
       "        (hidden_layer): Linear(original_name=Linear)\n",
       "        (output_layer): Linear(original_name=Linear)\n",
       "      )\n",
       "      (ls2): LayerScale(original_name=LayerScale)\n",
       "    )\n",
       "    (3): VisualEncoderBlock(\n",
       "      original_name=VisualEncoderBlock\n",
       "      (norm1): LayerNorm(original_name=LayerNorm)\n",
       "      (attn): Attention(\n",
       "        original_name=Attention\n",
       "        (query): Linear(original_name=Linear)\n",
       "        (key): Linear(original_name=Linear)\n",
       "        (value): Linear(original_name=Linear)\n",
       "        (out): Linear(original_name=Linear)\n",
       "      )\n",
       "      (ls1): LayerScale(original_name=LayerScale)\n",
       "      (norm2): LayerNorm(original_name=LayerNorm)\n",
       "      (mlp): MLP(\n",
       "        original_name=MLP\n",
       "        (hidden_layer): Linear(original_name=Linear)\n",
       "        (output_layer): Linear(original_name=Linear)\n",
       "      )\n",
       "      (ls2): LayerScale(original_name=LayerScale)\n",
       "    )\n",
       "    (4): VisualEncoderBlock(\n",
       "      original_name=VisualEncoderBlock\n",
       "      (norm1): LayerNorm(original_name=LayerNorm)\n",
       "      (attn): Attention(\n",
       "        original_name=Attention\n",
       "        (query): Linear(original_name=Linear)\n",
       "        (key): Linear(original_name=Linear)\n",
       "        (value): Linear(original_name=Linear)\n",
       "        (out): Linear(original_name=Linear)\n",
       "      )\n",
       "      (ls1): LayerScale(original_name=LayerScale)\n",
       "      (norm2): LayerNorm(original_name=LayerNorm)\n",
       "      (mlp): MLP(\n",
       "        original_name=MLP\n",
       "        (hidden_layer): Linear(original_name=Linear)\n",
       "        (output_layer): Linear(original_name=Linear)\n",
       "      )\n",
       "      (ls2): LayerScale(original_name=LayerScale)\n",
       "    )\n",
       "    (5): VisualEncoderBlock(\n",
       "      original_name=VisualEncoderBlock\n",
       "      (norm1): LayerNorm(original_name=LayerNorm)\n",
       "      (attn): Attention(\n",
       "        original_name=Attention\n",
       "        (query): Linear(original_name=Linear)\n",
       "        (key): Linear(original_name=Linear)\n",
       "        (value): Linear(original_name=Linear)\n",
       "        (out): Linear(original_name=Linear)\n",
       "      )\n",
       "      (ls1): LayerScale(original_name=LayerScale)\n",
       "      (norm2): LayerNorm(original_name=LayerNorm)\n",
       "      (mlp): MLP(\n",
       "        original_name=MLP\n",
       "        (hidden_layer): Linear(original_name=Linear)\n",
       "        (output_layer): Linear(original_name=Linear)\n",
       "      )\n",
       "      (ls2): LayerScale(original_name=LayerScale)\n",
       "    )\n",
       "    (6): VisualEncoderBlock(\n",
       "      original_name=VisualEncoderBlock\n",
       "      (norm1): LayerNorm(original_name=LayerNorm)\n",
       "      (attn): Attention(\n",
       "        original_name=Attention\n",
       "        (query): Linear(original_name=Linear)\n",
       "        (key): Linear(original_name=Linear)\n",
       "        (value): Linear(original_name=Linear)\n",
       "        (out): Linear(original_name=Linear)\n",
       "      )\n",
       "      (ls1): LayerScale(original_name=LayerScale)\n",
       "      (norm2): LayerNorm(original_name=LayerNorm)\n",
       "      (mlp): MLP(\n",
       "        original_name=MLP\n",
       "        (hidden_layer): Linear(original_name=Linear)\n",
       "        (output_layer): Linear(original_name=Linear)\n",
       "      )\n",
       "      (ls2): LayerScale(original_name=LayerScale)\n",
       "    )\n",
       "    (7): VisualEncoderBlock(\n",
       "      original_name=VisualEncoderBlock\n",
       "      (norm1): LayerNorm(original_name=LayerNorm)\n",
       "      (attn): Attention(\n",
       "        original_name=Attention\n",
       "        (query): Linear(original_name=Linear)\n",
       "        (key): Linear(original_name=Linear)\n",
       "        (value): Linear(original_name=Linear)\n",
       "        (out): Linear(original_name=Linear)\n",
       "      )\n",
       "      (ls1): LayerScale(original_name=LayerScale)\n",
       "      (norm2): LayerNorm(original_name=LayerNorm)\n",
       "      (mlp): MLP(\n",
       "        original_name=MLP\n",
       "        (hidden_layer): Linear(original_name=Linear)\n",
       "        (output_layer): Linear(original_name=Linear)\n",
       "      )\n",
       "      (ls2): LayerScale(original_name=LayerScale)\n",
       "    )\n",
       "    (8): VisualEncoderBlock(\n",
       "      original_name=VisualEncoderBlock\n",
       "      (norm1): LayerNorm(original_name=LayerNorm)\n",
       "      (attn): Attention(\n",
       "        original_name=Attention\n",
       "        (query): Linear(original_name=Linear)\n",
       "        (key): Linear(original_name=Linear)\n",
       "        (value): Linear(original_name=Linear)\n",
       "        (out): Linear(original_name=Linear)\n",
       "      )\n",
       "      (ls1): LayerScale(original_name=LayerScale)\n",
       "      (norm2): LayerNorm(original_name=LayerNorm)\n",
       "      (mlp): MLP(\n",
       "        original_name=MLP\n",
       "        (hidden_layer): Linear(original_name=Linear)\n",
       "        (output_layer): Linear(original_name=Linear)\n",
       "      )\n",
       "      (ls2): LayerScale(original_name=LayerScale)\n",
       "    )\n",
       "    (9): VisualEncoderBlock(\n",
       "      original_name=VisualEncoderBlock\n",
       "      (norm1): LayerNorm(original_name=LayerNorm)\n",
       "      (attn): Attention(\n",
       "        original_name=Attention\n",
       "        (query): Linear(original_name=Linear)\n",
       "        (key): Linear(original_name=Linear)\n",
       "        (value): Linear(original_name=Linear)\n",
       "        (out): Linear(original_name=Linear)\n",
       "      )\n",
       "      (ls1): LayerScale(original_name=LayerScale)\n",
       "      (norm2): LayerNorm(original_name=LayerNorm)\n",
       "      (mlp): MLP(\n",
       "        original_name=MLP\n",
       "        (hidden_layer): Linear(original_name=Linear)\n",
       "        (output_layer): Linear(original_name=Linear)\n",
       "      )\n",
       "      (ls2): LayerScale(original_name=LayerScale)\n",
       "    )\n",
       "    (10): VisualEncoderBlock(\n",
       "      original_name=VisualEncoderBlock\n",
       "      (norm1): LayerNorm(original_name=LayerNorm)\n",
       "      (attn): Attention(\n",
       "        original_name=Attention\n",
       "        (query): Linear(original_name=Linear)\n",
       "        (key): Linear(original_name=Linear)\n",
       "        (value): Linear(original_name=Linear)\n",
       "        (out): Linear(original_name=Linear)\n",
       "      )\n",
       "      (ls1): LayerScale(original_name=LayerScale)\n",
       "      (norm2): LayerNorm(original_name=LayerNorm)\n",
       "      (mlp): MLP(\n",
       "        original_name=MLP\n",
       "        (hidden_layer): Linear(original_name=Linear)\n",
       "        (output_layer): Linear(original_name=Linear)\n",
       "      )\n",
       "      (ls2): LayerScale(original_name=LayerScale)\n",
       "    )\n",
       "    (11): VisualEncoderBlock(\n",
       "      original_name=VisualEncoderBlock\n",
       "      (norm1): LayerNorm(original_name=LayerNorm)\n",
       "      (attn): Attention(\n",
       "        original_name=Attention\n",
       "        (query): Linear(original_name=Linear)\n",
       "        (key): Linear(original_name=Linear)\n",
       "        (value): Linear(original_name=Linear)\n",
       "        (out): Linear(original_name=Linear)\n",
       "      )\n",
       "      (ls1): LayerScale(original_name=LayerScale)\n",
       "      (norm2): LayerNorm(original_name=LayerNorm)\n",
       "      (mlp): MLP(\n",
       "        original_name=MLP\n",
       "        (hidden_layer): Linear(original_name=Linear)\n",
       "        (output_layer): Linear(original_name=Linear)\n",
       "      )\n",
       "      (ls2): LayerScale(original_name=LayerScale)\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm(original_name=LayerNorm)\n",
       "  (embedding_projection): Linear(original_name=Linear)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module = model.image_encoder\n",
    "module.eval()\n",
    "module.return_features = True\n",
    "\n",
    "traced_script_module = torch.jit.trace(module, example_inputs=image_data)\n",
    "traced_script_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting PyTorch Frontend ==> MIL Ops: 100%|█████████▉| 453/455 [00:00<00:00, 5638.83 ops/s]\n",
      "Running MIL frontend_pytorch pipeline: 100%|██████████| 5/5 [00:00<00:00, 381.07 passes/s]\n",
      "Running MIL default pipeline: 100%|██████████| 69/69 [00:00<00:00, 156.08 passes/s]\n",
      "Running MIL backend_mlprogram pipeline: 100%|██████████| 12/12 [00:00<00:00, 699.38 passes/s]\n"
     ]
    }
   ],
   "source": [
    "coreml_model = ct.convert(\n",
    "    traced_script_module, source=\"pytorch\",\n",
    "    inputs=[image_input], outputs=[image_features, image_embeddings],\n",
    "    convert_to='mlprogram', compute_precision=ct.precision.FLOAT32)\n",
    "\n",
    "coreml_model.author = 'Unum Cloud'\n",
    "coreml_model.license = 'Apache 2.0'\n",
    "coreml_model.short_description = 'Pocket-Sized Multimodal AI for Content Understanding'\n",
    "coreml_model.save(\"../uform-vl-english-small-image.mlpackage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextEncoder(\n",
       "  original_name=TextEncoder\n",
       "  (word_embeddings): Embedding(original_name=Embedding)\n",
       "  (position_embeddings): Embedding(original_name=Embedding)\n",
       "  (layer_norm): LayerNorm(original_name=LayerNorm)\n",
       "  (dropout): Dropout(original_name=Dropout)\n",
       "  (blocks): ModuleList(\n",
       "    original_name=ModuleList\n",
       "    (0): TextEncoderBlock(\n",
       "      original_name=TextEncoderBlock\n",
       "      (norm_attn): LayerNorm(original_name=LayerNorm)\n",
       "      (attention): Attention(\n",
       "        original_name=Attention\n",
       "        (query): Linear(original_name=Linear)\n",
       "        (key): Linear(original_name=Linear)\n",
       "        (value): Linear(original_name=Linear)\n",
       "        (out): Linear(original_name=Linear)\n",
       "      )\n",
       "      (norm_mlp): LayerNorm(original_name=LayerNorm)\n",
       "      (mlp): MLP(\n",
       "        original_name=MLP\n",
       "        (hidden_layer): Linear(original_name=Linear)\n",
       "        (output_layer): Linear(original_name=Linear)\n",
       "      )\n",
       "      (dropout): Dropout(original_name=Dropout)\n",
       "    )\n",
       "    (1): TextEncoderBlock(\n",
       "      original_name=TextEncoderBlock\n",
       "      (norm_attn): LayerNorm(original_name=LayerNorm)\n",
       "      (attention): Attention(\n",
       "        original_name=Attention\n",
       "        (query): Linear(original_name=Linear)\n",
       "        (key): Linear(original_name=Linear)\n",
       "        (value): Linear(original_name=Linear)\n",
       "        (out): Linear(original_name=Linear)\n",
       "      )\n",
       "      (norm_mlp): LayerNorm(original_name=LayerNorm)\n",
       "      (mlp): MLP(\n",
       "        original_name=MLP\n",
       "        (hidden_layer): Linear(original_name=Linear)\n",
       "        (output_layer): Linear(original_name=Linear)\n",
       "      )\n",
       "      (dropout): Dropout(original_name=Dropout)\n",
       "    )\n",
       "    (2): TextEncoderBlock(\n",
       "      original_name=TextEncoderBlock\n",
       "      (norm_attn): LayerNorm(original_name=LayerNorm)\n",
       "      (attention): Attention(\n",
       "        original_name=Attention\n",
       "        (query): Linear(original_name=Linear)\n",
       "        (key): Linear(original_name=Linear)\n",
       "        (value): Linear(original_name=Linear)\n",
       "        (out): Linear(original_name=Linear)\n",
       "      )\n",
       "      (norm_crossattn): LayerNorm(original_name=LayerNorm)\n",
       "      (crossattn): Attention(\n",
       "        original_name=Attention\n",
       "        (query): Linear(original_name=Linear)\n",
       "        (key): Linear(original_name=Linear)\n",
       "        (value): Linear(original_name=Linear)\n",
       "        (out): Linear(original_name=Linear)\n",
       "      )\n",
       "      (norm_mlp): LayerNorm(original_name=LayerNorm)\n",
       "      (mlp): MLP(\n",
       "        original_name=MLP\n",
       "        (hidden_layer): Linear(original_name=Linear)\n",
       "        (output_layer): Linear(original_name=Linear)\n",
       "      )\n",
       "      (dropout): Dropout(original_name=Dropout)\n",
       "    )\n",
       "    (3): TextEncoderBlock(\n",
       "      original_name=TextEncoderBlock\n",
       "      (norm_attn): LayerNorm(original_name=LayerNorm)\n",
       "      (attention): Attention(\n",
       "        original_name=Attention\n",
       "        (query): Linear(original_name=Linear)\n",
       "        (key): Linear(original_name=Linear)\n",
       "        (value): Linear(original_name=Linear)\n",
       "        (out): Linear(original_name=Linear)\n",
       "      )\n",
       "      (norm_crossattn): LayerNorm(original_name=LayerNorm)\n",
       "      (crossattn): Attention(\n",
       "        original_name=Attention\n",
       "        (query): Linear(original_name=Linear)\n",
       "        (key): Linear(original_name=Linear)\n",
       "        (value): Linear(original_name=Linear)\n",
       "        (out): Linear(original_name=Linear)\n",
       "      )\n",
       "      (norm_mlp): LayerNorm(original_name=LayerNorm)\n",
       "      (mlp): MLP(\n",
       "        original_name=MLP\n",
       "        (hidden_layer): Linear(original_name=Linear)\n",
       "        (output_layer): Linear(original_name=Linear)\n",
       "      )\n",
       "      (dropout): Dropout(original_name=Dropout)\n",
       "    )\n",
       "  )\n",
       "  (embedding_projection): Linear(original_name=Linear)\n",
       "  (matching_head): Linear(original_name=Linear)\n",
       "  (context_projection): Linear(original_name=Linear)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module = model.text_encoder\n",
    "module.eval()\n",
    "module.return_features = True\n",
    "\n",
    "traced_script_module = torch.jit.trace(module, example_inputs=[text_data['input_ids'], text_data['attention_mask']])\n",
    "traced_script_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tuple detected at graph output. This will be flattened in the converted model.\n",
      "Converting PyTorch Frontend ==> MIL Ops:   0%|          | 0/157 [00:00<?, ? ops/s]Core ML embedding (gather) layer does not support any inputs besides the weights and indices. Those given will be ignored.\n",
      "Converting PyTorch Frontend ==> MIL Ops:  99%|█████████▊| 155/157 [00:00<00:00, 6809.29 ops/s]\n",
      "Running MIL frontend_pytorch pipeline: 100%|██████████| 5/5 [00:00<00:00, 1947.76 passes/s]\n",
      "Running MIL default pipeline: 100%|██████████| 69/69 [00:00<00:00, 816.08 passes/s]\n",
      "Running MIL backend_mlprogram pipeline: 100%|██████████| 12/12 [00:00<00:00, 3294.17 passes/s]\n"
     ]
    }
   ],
   "source": [
    "coreml_model = ct.convert(\n",
    "    traced_script_module, source=\"pytorch\",\n",
    "    inputs=[text_input, text_attention_input], outputs=[text_features, text_embeddings],\n",
    "    convert_to='mlprogram', compute_precision=ct.precision.FLOAT32)\n",
    "\n",
    "coreml_model.author = 'Unum Cloud'\n",
    "coreml_model.license = 'Apache 2.0'\n",
    "coreml_model.short_description = 'Pocket-Sized Multimodal AI for Content Understanding'\n",
    "coreml_model.save(\"../uform-vl-english-small-text.mlpackage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
